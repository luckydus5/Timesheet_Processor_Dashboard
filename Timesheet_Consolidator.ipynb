{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a194e7cf",
   "metadata": {},
   "source": [
    "# üßπ **TIMESHEET CONSOLIDATOR & BUSINESS RULES PROCESSOR**\n",
    "## Professional Excel/CSV Timesheet Data Cleaning System\n",
    "\n",
    "This notebook automatically handles:\n",
    "- **Multiple check-ins/check-outs per employee per date**\n",
    "- **Consolidates duplicate entries into single rows**\n",
    "- **Applies your exact business rules**\n",
    "- **Handles Day/Night shift determination**\n",
    "- **Calculates overtime with company rules**\n",
    "\n",
    "### üéØ **Business Rules Applied:**\n",
    "- **Day Shift**: Official 8:00 AM - 17:00 PM (can check-in early, no OT for early arrival)\n",
    "- **Night Shift**: Official 18:00 PM - 3:00 AM (can check-in early, no OT for early arrival)\n",
    "- **Start Time**: FIRST check-in (C/In or OverTime In) per employee per date\n",
    "- **End Time**: LAST check-out (C/Out or OverTime Out) per employee per date\n",
    "- **Overtime**: Day shift after 17:00 PM (30min-1.5h), Night shift after 3:00 AM (30min-3h)\n",
    "\n",
    "### üìã **Usage**: Simply update the file path and run all cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932a97f",
   "metadata": {},
   "source": [
    "## üì¶ **Step 1: Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2cedba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üöÄ Ready to process timesheet data!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, time, timedelta\n",
    "import warnings\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üöÄ Ready to process timesheet data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361a81e",
   "metadata": {},
   "source": [
    "## üìÇ **Step 2: Load Your Timesheet File**\n",
    "\n",
    "**üîß CONFIGURATION: Just enter your filename below - the system automatically looks in the Data Cleaner folder**\n",
    "\n",
    "**üìÅ Auto-Path: `/home/luckdus/Desktop/Data Cleaner/`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "500e7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading timesheet data: 88888888.xlsx\n",
      "üìÅ From folder: /home/luckdus/Desktop/Data Cleaner\n",
      "‚úÖ Excel file loaded: 88888888.xlsx\n",
      "üìÅ Full path: /home/luckdus/Desktop/Data Cleaner/88888888.xlsx\n",
      "\n",
      "üìä Data Overview:\n",
      "   - Total records: 2,500\n",
      "   - Columns: ['Department', 'Name', 'No.', 'Date/Time', 'Status', 'Location ID', 'ID Number', 'Workcode', 'VerifyCode', 'CardNo']\n",
      "üîÑ Detected combined Date/Time column - splitting...\n",
      "‚úÖ Successfully split Date/Time into separate columns\n",
      "‚úÖ All required columns present\n",
      "\n",
      "üìã First 5 records:\n",
      "‚úÖ Excel file loaded: 88888888.xlsx\n",
      "üìÅ Full path: /home/luckdus/Desktop/Data Cleaner/88888888.xlsx\n",
      "\n",
      "üìä Data Overview:\n",
      "   - Total records: 2,500\n",
      "   - Columns: ['Department', 'Name', 'No.', 'Date/Time', 'Status', 'Location ID', 'ID Number', 'Workcode', 'VerifyCode', 'CardNo']\n",
      "üîÑ Detected combined Date/Time column - splitting...\n",
      "‚úÖ Successfully split Date/Time into separate columns\n",
      "‚úÖ All required columns present\n",
      "\n",
      "üìã First 5 records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hategekimanaalice</td>\n",
       "      <td>08/01/2025</td>\n",
       "      <td>06:43:19</td>\n",
       "      <td>OverTime In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hategekimanaalice</td>\n",
       "      <td>08/01/2025</td>\n",
       "      <td>17:08:54</td>\n",
       "      <td>C/Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hategekimanaalice</td>\n",
       "      <td>08/02/2025</td>\n",
       "      <td>06:44:58</td>\n",
       "      <td>OverTime In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hategekimanaalice</td>\n",
       "      <td>08/02/2025</td>\n",
       "      <td>16:54:40</td>\n",
       "      <td>C/Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hategekimanaalice</td>\n",
       "      <td>08/03/2025</td>\n",
       "      <td>08:21:17</td>\n",
       "      <td>OverTime In</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name        Date      Time       Status\n",
       "0  Hategekimanaalice  08/01/2025  06:43:19  OverTime In\n",
       "1  Hategekimanaalice  08/01/2025  17:08:54        C/Out\n",
       "2  Hategekimanaalice  08/02/2025  06:44:58  OverTime In\n",
       "3  Hategekimanaalice  08/02/2025  16:54:40        C/Out\n",
       "4  Hategekimanaalice  08/03/2025  08:21:17  OverTime In"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üîß UPDATE THIS WITH YOUR FILENAME ONLY (NOT FULL PATH)\n",
    "FILE_NAME = \"88888888.xlsx\"  # Just enter the filename - system will find it automatically\n",
    "\n",
    "# System automatically looks in this folder:\n",
    "BASE_FOLDER = \"/home/luckdus/Desktop/Data Cleaner\"\n",
    "\n",
    "def load_timesheet_file(file_name, base_folder=BASE_FOLDER):\n",
    "    \"\"\"Load timesheet data from Excel or CSV file\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create full file path automatically\n",
    "    file_path = os.path.join(base_folder, file_name)\n",
    "    \n",
    "    try:\n",
    "        # Determine file type and load accordingly\n",
    "        if file_name.lower().endswith('.xlsx') or file_name.lower().endswith('.xls'):\n",
    "            df = pd.read_excel(file_path)\n",
    "            print(f\"‚úÖ Excel file loaded: {file_name}\")\n",
    "        elif file_name.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"‚úÖ CSV file loaded: {file_name}\")\n",
    "        else:\n",
    "            raise ValueError(\"File must be Excel (.xlsx/.xls) or CSV (.csv)\")\n",
    "        \n",
    "        print(f\"üìÅ Full path: {file_path}\")\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"\\nüìä Data Overview:\")\n",
    "        print(f\"   - Total records: {len(df):,}\")\n",
    "        print(f\"   - Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Handle different file formats\n",
    "        if 'Date/Time' in df.columns:\n",
    "            print(f\"üîÑ Detected combined Date/Time column - splitting...\")\n",
    "            \n",
    "            # Split Date/Time column into Date and Time\n",
    "            df['DateTime_parsed'] = pd.to_datetime(df['Date/Time'], errors='coerce')\n",
    "            df['Date'] = df['DateTime_parsed'].dt.strftime('%d/%m/%Y')\n",
    "            df['Time'] = df['DateTime_parsed'].dt.strftime('%H:%M:%S')\n",
    "            \n",
    "            print(f\"‚úÖ Successfully split Date/Time into separate columns\")\n",
    "        \n",
    "        # Check for required columns (after potential splitting)\n",
    "        required_cols = ['Name', 'Date', 'Time', 'Status']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "            print(f\"üí° Available columns: {list(df.columns)}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüìã First 5 records:\")\n",
    "        display(df[['Name', 'Date', 'Time', 'Status']].head())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_name}\")\n",
    "        print(f\"‚ùå Looked in: {base_folder}\")\n",
    "        print(f\"üí° Make sure the file exists in the Data Cleaner folder\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "print(f\"üöÄ Loading timesheet data: {FILE_NAME}\")\n",
    "print(f\"üìÅ From folder: {BASE_FOLDER}\")\n",
    "raw_data = load_timesheet_file(FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a8290",
   "metadata": {},
   "source": [
    "## üîç **Step 3: Analyze Duplicate Entries**\n",
    "\n",
    "Let's first understand the duplicate entries problem in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f85791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ANALYZING DUPLICATE ENTRIES\n",
      "==================================================\n",
      "üìä Duplicate Entry Analysis:\n",
      "   - Total unique employee-date combinations: 465\n",
      "   - Employee-dates with multiple entries: 434\n",
      "   - Percentage with duplicates: 93.3%\n",
      "\n",
      "üìà Entry Count Distribution:\n",
      "   1 entries per day: 31 employee-dates\n",
      "   2 entries per day: 374 employee-dates\n",
      "   3 entries per day: 28 employee-dates\n",
      "   4 entries per day: 30 employee-dates\n",
      "   5 entries per day: 2 employee-dates\n",
      "\n",
      "üìù Examples of Multiple Entries:\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/04/2025 (5 entries):\n",
      "      06:46:49  - OverTime Out\n",
      "      06:47:34  - OverTime In \n",
      "      07:41:57  - C/In        \n",
      "      17:00:08  - OverTime Out\n",
      "      17:02:42  - C/Out       \n",
      "\n",
      "   üë§ TUYISHIMIRE DIEUDONNE on 08/11/2025 (5 entries):\n",
      "      06:43:05  - OverTime In \n",
      "      07:40:47  - C/In        \n",
      "      17:04:58  - C/Out       \n",
      "      18:03:22  - OverTime Out\n",
      "      18:15:34  - OverTime Out\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/01/2025 (4 entries):\n",
      "      06:44:57  - C/In        \n",
      "      07:39:12  - C/In        \n",
      "      17:03:45  - C/Out       \n",
      "      17:37:20  - OverTime Out\n",
      "\n",
      "‚úÖ Analysis complete - Ready for consolidation!\n"
     ]
    }
   ],
   "source": [
    "if raw_data is not None:\n",
    "    print(\"üîç ANALYZING DUPLICATE ENTRIES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Count entries per employee per date\n",
    "    duplicate_analysis = raw_data.groupby(['Name', 'Date']).size().reset_index(name='Entry_Count')\n",
    "    \n",
    "    # Find employees with multiple entries per date\n",
    "    multiple_entries = duplicate_analysis[duplicate_analysis['Entry_Count'] > 1]\n",
    "    \n",
    "    print(f\"üìä Duplicate Entry Analysis:\")\n",
    "    print(f\"   - Total unique employee-date combinations: {len(duplicate_analysis):,}\")\n",
    "    print(f\"   - Employee-dates with multiple entries: {len(multiple_entries):,}\")\n",
    "    print(f\"   - Percentage with duplicates: {len(multiple_entries)/len(duplicate_analysis)*100:.1f}%\")\n",
    "    \n",
    "    # Show distribution of entry counts\n",
    "    entry_distribution = duplicate_analysis['Entry_Count'].value_counts().sort_index()\n",
    "    print(f\"\\nüìà Entry Count Distribution:\")\n",
    "    for count, frequency in entry_distribution.items():\n",
    "        print(f\"   {count} entries per day: {frequency:,} employee-dates\")\n",
    "    \n",
    "    # Show examples of problematic cases\n",
    "    print(f\"\\nüìù Examples of Multiple Entries:\")\n",
    "    \n",
    "    # Show top 3 cases with most entries\n",
    "    top_cases = multiple_entries.nlargest(3, 'Entry_Count')\n",
    "    \n",
    "    for _, case in top_cases.iterrows():\n",
    "        name = case['Name']\n",
    "        date = case['Date']\n",
    "        count = case['Entry_Count']\n",
    "        \n",
    "        print(f\"\\n   üë§ {name} on {date} ({count} entries):\")\n",
    "        \n",
    "        # Show all entries for this employee-date\n",
    "        entries = raw_data[(raw_data['Name'] == name) & (raw_data['Date'] == date)].sort_values('Time')\n",
    "        for _, entry in entries.iterrows():\n",
    "            print(f\"      {entry['Time']:9} - {entry['Status']:12}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete - Ready for consolidation!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data loaded. Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea27ba8d",
   "metadata": {},
   "source": [
    "## üßπ **Step 4: Business Rules & Consolidation Functions**\n",
    "\n",
    "These functions implement your exact business rules for handling multiple entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Business rules functions defined!\n",
      "üéØ Ready to consolidate multiple entries per employee per date!\n"
     ]
    }
   ],
   "source": [
    "def parse_date_time(date_str, time_str):\n",
    "    \"\"\"Parse separate date and time strings\"\"\"\n",
    "    if pd.isna(date_str) or pd.isna(time_str) or date_str == '' or time_str == '':\n",
    "        return None, None\n",
    "    try:\n",
    "        # Parse date string (various formats supported, always output DD/MM/YYYY)\n",
    "        date_obj = pd.to_datetime(date_str, dayfirst=True).date()\n",
    "        \n",
    "        # Parse time string\n",
    "        time_obj = pd.to_datetime(time_str, format='%H:%M:%S').time()\n",
    "        \n",
    "        return date_obj, time_obj\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def format_hours_as_time(hours):\n",
    "    \"\"\"Convert decimal hours to HH:MM format\"\"\"\n",
    "    if hours == 0:\n",
    "        return \"0:00\"\n",
    "    \n",
    "    # Extract whole hours\n",
    "    whole_hours = int(hours)\n",
    "    \n",
    "    # Extract minutes from decimal part\n",
    "    minutes_decimal = (hours - whole_hours) * 60\n",
    "    whole_minutes = int(minutes_decimal)\n",
    "    \n",
    "    return f\"{whole_hours}:{whole_minutes:02d}\"\n",
    "\n",
    "def detect_cross_midnight_shifts(df):\n",
    "    \"\"\"\n",
    "    Enhanced cross-midnight shift detection\n",
    "    \n",
    "    Detects multiple patterns:\n",
    "    1. Direct: OverTime In (evening) ‚Üí OverTime Out (next morning)\n",
    "    2. Orphaned night shifts: OverTime In (evening) without immediate Out on same date\n",
    "    3. Orphaned morning outs: OverTime Out (morning) without In on same date\n",
    "    \n",
    "    Groups them intelligently based on time patterns and employee work sequences\n",
    "    \"\"\"\n",
    "    df_work = df.copy()\n",
    "    df_work['Shift_Group'] = df_work['Date_parsed']  # Default: group by original date\n",
    "    df_work['Processed'] = False  # Track which entries we've processed\n",
    "    \n",
    "    # Sort by employee, date, and time\n",
    "    df_work = df_work.sort_values(['Name', 'Date_parsed', 'Time_parsed'])\n",
    "    \n",
    "    employees = df_work['Name'].unique()\n",
    "    cross_midnight_count = 0\n",
    "    \n",
    "    for employee in employees:\n",
    "        emp_data = df_work[df_work['Name'] == employee].copy()\n",
    "        emp_indices = df_work[df_work['Name'] == employee].index.tolist()\n",
    "        \n",
    "        # STEP 1: Find direct cross-midnight patterns (In ‚Üí Out next day)\n",
    "        for i in range(len(emp_data) - 1):\n",
    "            current_idx = emp_indices[i]\n",
    "            next_idx = emp_indices[i + 1]\n",
    "            \n",
    "            current_row = emp_data.iloc[i]\n",
    "            next_row = emp_data.iloc[i + 1]\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if df_work.loc[current_idx, 'Processed'] or df_work.loc[next_idx, 'Processed']:\n",
    "                continue\n",
    "            \n",
    "            # Check for cross-midnight pattern\n",
    "            if (current_row['Status'] in ['OverTime In'] and \n",
    "                next_row['Status'] in ['OverTime Out'] and\n",
    "                current_row['Date_parsed'] != next_row['Date_parsed']):\n",
    "                \n",
    "                days_diff = (next_row['Date_parsed'] - current_row['Date_parsed']).days\n",
    "                \n",
    "                if days_diff == 1:\n",
    "                    current_time_decimal = current_row['Time_parsed'].hour + current_row['Time_parsed'].minute/60\n",
    "                    next_time_decimal = next_row['Time_parsed'].hour + next_row['Time_parsed'].minute/60\n",
    "                    \n",
    "                    # Night shift pattern: start in evening (16:00+), end in morning (before 12:00)\n",
    "                    if current_time_decimal >= 16.0 and next_time_decimal <= 12.0:\n",
    "                        # Group both entries under the START date\n",
    "                        df_work.loc[next_idx, 'Shift_Group'] = current_row['Date_parsed']\n",
    "                        df_work.loc[current_idx, 'Processed'] = True\n",
    "                        df_work.loc[next_idx, 'Processed'] = True\n",
    "                        cross_midnight_count += 2\n",
    "                        print(f\"üåô Direct cross-midnight: {employee} {current_row['Date_parsed']} ‚Üí {next_row['Date_parsed']}\")\n",
    "        \n",
    "        # STEP 2: Find orphaned evening check-ins (night shift starts)\n",
    "        for i in range(len(emp_data)):\n",
    "            current_idx = emp_indices[i]\n",
    "            current_row = emp_data.iloc[i]\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if df_work.loc[current_idx, 'Processed']:\n",
    "                continue\n",
    "            \n",
    "            # Look for evening OverTime In that might be start of night shift\n",
    "            if current_row['Status'] == 'OverTime In':\n",
    "                current_time_decimal = current_row['Time_parsed'].hour + current_row['Time_parsed'].minute/60\n",
    "                \n",
    "                # Evening check-in (16:00 or later)\n",
    "                if current_time_decimal >= 16.0:\n",
    "                    # Look for matching Out on next day(s)\n",
    "                    current_date = current_row['Date_parsed']\n",
    "                    \n",
    "                    # Search next few days for matching checkout\n",
    "                    for j in range(i + 1, min(i + 4, len(emp_data))):  # Look up to 3 days ahead\n",
    "                        next_idx = emp_indices[j]\n",
    "                        next_row = emp_data.iloc[j]\n",
    "                        \n",
    "                        # Skip if already processed\n",
    "                        if df_work.loc[next_idx, 'Processed']:\n",
    "                            continue\n",
    "                        \n",
    "                        # Look for morning checkout\n",
    "                        if (next_row['Status'] in ['OverTime Out'] and \n",
    "                            next_row['Date_parsed'] > current_date):\n",
    "                            \n",
    "                            next_time_decimal = next_row['Time_parsed'].hour + next_row['Time_parsed'].minute/60\n",
    "                            days_diff = (next_row['Date_parsed'] - current_date).days\n",
    "                            \n",
    "                            # Morning checkout (before 12:00) within reasonable time\n",
    "                            if next_time_decimal <= 12.0 and days_diff <= 2:\n",
    "                                # Group checkout under the check-in date\n",
    "                                df_work.loc[next_idx, 'Shift_Group'] = current_date\n",
    "                                df_work.loc[current_idx, 'Processed'] = True\n",
    "                                df_work.loc[next_idx, 'Processed'] = True\n",
    "                                cross_midnight_count += 2\n",
    "                                print(f\"üåô Orphaned night shift: {employee} {current_date} ‚Üí {next_row['Date_parsed']}\")\n",
    "                                break\n",
    "        \n",
    "        # STEP 3: Handle remaining orphaned morning checkouts\n",
    "        for i in range(len(emp_data)):\n",
    "            current_idx = emp_indices[i]\n",
    "            current_row = emp_data.iloc[i]\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if df_work.loc[current_idx, 'Processed']:\n",
    "                continue\n",
    "            \n",
    "            # Look for morning OverTime Out that might be end of night shift\n",
    "            if current_row['Status'] == 'OverTime Out':\n",
    "                current_time_decimal = current_row['Time_parsed'].hour + current_row['Time_parsed'].minute/60\n",
    "                \n",
    "                # Morning checkout (before 12:00)\n",
    "                if current_time_decimal <= 12.0:\n",
    "                    current_date = current_row['Date_parsed']\n",
    "                    \n",
    "                    # Look for matching In on previous day(s)\n",
    "                    for j in range(i - 1, max(i - 4, -1), -1):  # Look up to 3 days back\n",
    "                        prev_idx = emp_indices[j]\n",
    "                        prev_row = emp_data.iloc[j]\n",
    "                        \n",
    "                        # Skip if already processed\n",
    "                        if df_work.loc[prev_idx, 'Processed']:\n",
    "                            continue\n",
    "                        \n",
    "                        # Look for evening check-in\n",
    "                        if (prev_row['Status'] in ['OverTime In'] and \n",
    "                            prev_row['Date_parsed'] < current_date):\n",
    "                            \n",
    "                            prev_time_decimal = prev_row['Time_parsed'].hour + prev_row['Time_parsed'].minute/60\n",
    "                            days_diff = (current_date - prev_row['Date_parsed']).days\n",
    "                            \n",
    "                            # Evening check-in (16:00+) within reasonable time\n",
    "                            if prev_time_decimal >= 16.0 and days_diff <= 2:\n",
    "                                # Group checkout under the check-in date\n",
    "                                df_work.loc[current_idx, 'Shift_Group'] = prev_row['Date_parsed']\n",
    "                                df_work.loc[prev_idx, 'Processed'] = True\n",
    "                                df_work.loc[current_idx, 'Processed'] = True\n",
    "                                cross_midnight_count += 2\n",
    "                                print(f\"üåô Orphaned morning out: {employee} {prev_row['Date_parsed']} ‚Üí {current_date}\")\n",
    "                                break\n",
    "    \n",
    "    # Clean up the temporary column\n",
    "    df_work = df_work.drop('Processed', axis=1)\n",
    "    \n",
    "    if cross_midnight_count > 0:\n",
    "        print(f\"‚úÖ Total cross-midnight entries processed: {cross_midnight_count}\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "def find_first_checkin_last_checkout(employee_day_records):\n",
    "    \"\"\"\n",
    "    Find FIRST check-in and LAST check-out for an employee on a specific date\n",
    "    \n",
    "    Business Rule:\n",
    "    - Start Time = FIRST check-in (C/In or OverTime In)\n",
    "    - End Time = LAST check-out (C/Out or OverTime Out)\n",
    "    \"\"\"\n",
    "    if employee_day_records.empty:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Sort by time to get chronological order\n",
    "    sorted_records = employee_day_records.sort_values('Time_parsed')\n",
    "    \n",
    "    # Find all check-ins (C/In and OverTime In)\n",
    "    checkins = sorted_records[sorted_records['Status'].isin(['C/In', 'OverTime In'])]\n",
    "    \n",
    "    # Find all check-outs (C/Out and OverTime Out)\n",
    "    checkouts = sorted_records[sorted_records['Status'].isin(['C/Out', 'OverTime Out'])]\n",
    "    \n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "    \n",
    "    # Get FIRST check-in\n",
    "    if not checkins.empty:\n",
    "        first_checkin = checkins.iloc[0]\n",
    "        start_time = first_checkin['Time_parsed']\n",
    "        start_date = first_checkin['Date_parsed']\n",
    "    \n",
    "    # Get LAST check-out\n",
    "    if not checkouts.empty:\n",
    "        last_checkout = checkouts.iloc[-1]\n",
    "        end_time = last_checkout['Time_parsed']\n",
    "        end_date = last_checkout['Date_parsed']\n",
    "    \n",
    "    return start_time, end_time, start_date, end_date\n",
    "\n",
    "def determine_shift_type(start_time, end_time, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Determine shift type based on check-in and check-out times\n",
    "    \n",
    "    Business Rules:\n",
    "    - Day Shift: Generally works 8:00 AM - 17:00 PM\n",
    "    - Night Shift: Official 18:00 PM - 3:00 AM (but workers can check in as early as 16:20 PM)\n",
    "    - Cross-midnight shifts: When start_date != end_date\n",
    "    \n",
    "    Logic:\n",
    "    1. If start_date != end_date = Likely Night Shift (cross-midnight)\n",
    "    2. If check-in is 16:20 PM or later AND check-out suggests night work = Night Shift\n",
    "    3. If check-in is between 6:00 AM and 4:19 PM (16:19) = Day Shift\n",
    "    \"\"\"\n",
    "    if start_time is None:\n",
    "        return \"\"\n",
    "    \n",
    "    start_hour = start_time.hour\n",
    "    start_minute = start_time.minute\n",
    "    end_hour = end_time.hour if end_time else start_hour\n",
    "    end_minute = end_time.minute if end_time else 0\n",
    "    \n",
    "    # Convert to decimal hours for easier comparison\n",
    "    start_decimal = start_hour + start_minute/60\n",
    "    end_decimal = end_hour + end_minute/60 if end_time else start_decimal\n",
    "    \n",
    "    # Case 1: Cross-midnight shift (start and end on different dates)\n",
    "    if start_date and end_date and start_date != end_date:\n",
    "        # If shift spans multiple dates, it's almost certainly a night shift\n",
    "        return \"Night Shift\"\n",
    "    \n",
    "    # Case 2: Early Night Shift Detection (16:20 PM or later)\n",
    "    # Check if start time is 16:20 (16.33) or later\n",
    "    if start_decimal >= 16.33:  # 16:20 PM = 16.33 in decimal\n",
    "        # If check-in is after 16:20 PM, likely night shift\n",
    "        # Verify with check-out time if available\n",
    "        if end_time:\n",
    "            # If check-out is late night/early morning (18:00+ or 00:00-06:00), definitely night shift\n",
    "            if end_decimal >= 18.0 or end_decimal <= 6.0:\n",
    "                return \"Night Shift\"\n",
    "            # If check-out is very late same day (after 20:00), likely night shift\n",
    "            elif end_decimal >= 20.0:\n",
    "                return \"Night Shift\"\n",
    "        # Even without clear check-out, assume night shift if check-in >= 16:20\n",
    "        return \"Night Shift\"\n",
    "    \n",
    "    # Case 3: Clear Day Shift (check-in between 6:00 AM and 4:19 PM)\n",
    "    elif 6.0 <= start_decimal < 16.33:\n",
    "        # Verify it's not a night shift ending in the morning\n",
    "        if end_time and 0.0 <= end_decimal <= 6.0:\n",
    "            # If check-out is early morning, might be night shift ending\n",
    "            return \"Night Shift\"\n",
    "        else:\n",
    "            return \"Day Shift\"\n",
    "    \n",
    "    # Case 4: Late Night Shift (18:00 PM or later)\n",
    "    elif start_decimal >= 18.0:\n",
    "        return \"Night Shift\"\n",
    "    \n",
    "    # Case 5: Very early morning check-in (00:00 - 05:59)\n",
    "    elif 0.0 <= start_decimal < 6.0:\n",
    "        # If both check-in and check-out are in early morning, likely night shift ending\n",
    "        if end_time and 0.0 <= end_decimal <= 12.0:\n",
    "            return \"Night Shift\"\n",
    "        # If check-out is later in the day, might be very early day shift\n",
    "        else:\n",
    "            return \"Day Shift\"\n",
    "    \n",
    "    # Default case (shouldn't happen, but safety)\n",
    "    return \"Day Shift\"\n",
    "\n",
    "def calculate_total_work_hours(start_time, end_time, start_date, end_date, shift_type):\n",
    "    \"\"\"\n",
    "    Calculate total work hours between start and end time\n",
    "    Handles cross-midnight shifts for night workers\n",
    "    \"\"\"\n",
    "    if start_time is None or end_time is None:\n",
    "        return 0\n",
    "    \n",
    "    # Create full datetime objects\n",
    "    start_dt = datetime.combine(start_date, start_time)\n",
    "    \n",
    "    # Handle cross-midnight shifts\n",
    "    if start_date != end_date:\n",
    "        # Use actual end date for cross-midnight shifts\n",
    "        end_dt = datetime.combine(end_date, end_time)\n",
    "    else:\n",
    "        # Same day shift\n",
    "        end_dt = datetime.combine(start_date, end_time)\n",
    "        \n",
    "        # Handle case where end time is earlier than start time (cross-midnight on same date grouping)\n",
    "        if shift_type == \"Night Shift\" and end_time < start_time:\n",
    "            # Add one day to end time for cross-midnight calculation\n",
    "            end_dt += timedelta(days=1)\n",
    "    \n",
    "    # Calculate total hours\n",
    "    total_duration = end_dt - start_dt\n",
    "    total_hours = total_duration.total_seconds() / 3600\n",
    "    \n",
    "    return round(total_hours, 2)\n",
    "\n",
    "def calculate_overtime_hours(start_time, end_time, start_date, end_date, shift_type):\n",
    "    \"\"\"\n",
    "    Calculate overtime hours based on company business rules\n",
    "    \n",
    "    Day Shift Rules:\n",
    "    - NO overtime for early check-in (before 8:00 AM)\n",
    "    - Overtime only AFTER 17:00 PM (5:00 PM)\n",
    "    - Minimum: 30 minutes, Maximum: 1.5 hours\n",
    "    \n",
    "    Night Shift Rules:\n",
    "    - NO overtime for early check-in (before 18:00 PM, even if they come at 16:20)\n",
    "    - Overtime only AFTER 3:00 AM (next day)\n",
    "    - Minimum: 30 minutes, Maximum: 3 hours\n",
    "    \"\"\"\n",
    "    if start_time is None or end_time is None or shift_type == \"\":\n",
    "        return 0\n",
    "    \n",
    "    overtime = 0\n",
    "    \n",
    "    if shift_type == \"Day Shift\":\n",
    "        # Day shift overtime: ONLY after 17:00 PM (5:00 PM)\n",
    "        end_decimal = end_time.hour + end_time.minute/60 + end_time.second/3600\n",
    "        \n",
    "        if end_decimal > 17.0:  # After 5:00 PM\n",
    "            overtime = end_decimal - 17.0\n",
    "            \n",
    "            # Apply minimum 30 minutes rule\n",
    "            if overtime < 0.5:\n",
    "                overtime = 0\n",
    "            # Apply maximum 1.5 hours rule\n",
    "            elif overtime > 1.5:\n",
    "                overtime = 1.5\n",
    "                \n",
    "    elif shift_type == \"Night Shift\":\n",
    "        # Night shift overtime: ONLY after 3:00 AM (next day)\n",
    "        end_decimal = end_time.hour + end_time.minute/60 + end_time.second/3600\n",
    "        \n",
    "        # For night shift, check if end time is in early morning hours (cross-midnight)\n",
    "        if end_decimal <= 12.0:  # Early morning hours (00:00-12:00)\n",
    "            if end_decimal > 3.0:  # After 3:00 AM\n",
    "                overtime = end_decimal - 3.0\n",
    "                \n",
    "                # Apply minimum 30 minutes rule\n",
    "                if overtime < 0.5:\n",
    "                    overtime = 0\n",
    "                # Apply maximum 3 hours rule\n",
    "                elif overtime > 3.0:\n",
    "                    overtime = 3.0\n",
    "    \n",
    "    return round(overtime, 2)\n",
    "\n",
    "def calculate_regular_hours(total_hours, overtime_hours):\n",
    "    \"\"\"Calculate regular hours (total - overtime)\"\"\"\n",
    "    if total_hours == 0:\n",
    "        return 0\n",
    "    \n",
    "    regular = total_hours - overtime_hours\n",
    "    return round(max(regular, 0), 2)  # Ensure non-negative\n",
    "\n",
    "print(\"‚úÖ Business rules functions defined!\")\n",
    "print(\"üéØ Ready to consolidate multiple entries per employee per date!\")\n",
    "print(\"üîß ENHANCED cross-midnight shift detection:\")\n",
    "print(\"   üìÖ Day Shift: Check-in 6:00 AM - 4:19 PM (16:19)\")\n",
    "print(\"   üåô Night Shift: Check-in 4:20 PM (16:20) or later - detects early night shift workers!\")\n",
    "print(\"   üïê Official Night Shift: 18:00 PM - 3:00 AM (but recognizes early arrivals from 16:20)\")\n",
    "print(\"   üåÉ Cross-Midnight: Detects ALL patterns including orphaned entries!\")\n",
    "print(\"   üîÑ Shift Changes: Handles day‚Üínight and night‚Üíday transitions properly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3b4c6",
   "metadata": {},
   "source": [
    "## üöÄ **Step 5: Consolidate Data & Apply Business Rules**\n",
    "\n",
    "This is the main processing function that consolidates multiple entries into single rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting consolidation process...\n",
      "üßπ STARTING TIMESHEET CONSOLIDATION\n",
      "==================================================\n",
      "üìã Step 1: Preparing data structure...\n",
      "üìÖ Step 2: Parsing Date and Time...\n",
      "   ‚úÖ Successfully parsed 993 records (1507 failed)\n",
      "üîÑ Step 3: Consolidating multiple entries...\n",
      "   üìä Processing 465 unique employee-date combinations...\n",
      "   ‚úÖ Successfully parsed 993 records (1507 failed)\n",
      "üîÑ Step 3: Consolidating multiple entries...\n",
      "   üìä Processing 465 unique employee-date combinations...\n",
      "   üìà Processed 200/465 employee-date combinations...\n",
      "   üìà Processed 200/465 employee-date combinations...\n",
      "   üìà Processed 400/465 employee-date combinations...\n",
      "   ‚úÖ Completed consolidation: 428 unique shifts created\n",
      "üìä Step 4: Creating final consolidated dataset...\n",
      "‚úÖ Consolidation completed successfully!\n",
      "\n",
      "üìä CONSOLIDATION SUMMARY:\n",
      "   - Original records: 2,500\n",
      "   - Consolidated records: 428\n",
      "   - Reduction: 2,072 duplicate entries removed\n",
      "   - Unique employees: 42\n",
      "   - Days with multiple entries: 428\n",
      "   - Average entries per day: 2.2\n",
      "   üìà Processed 400/465 employee-date combinations...\n",
      "   ‚úÖ Completed consolidation: 428 unique shifts created\n",
      "üìä Step 4: Creating final consolidated dataset...\n",
      "‚úÖ Consolidation completed successfully!\n",
      "\n",
      "üìä CONSOLIDATION SUMMARY:\n",
      "   - Original records: 2,500\n",
      "   - Consolidated records: 428\n",
      "   - Reduction: 2,072 duplicate entries removed\n",
      "   - Unique employees: 42\n",
      "   - Days with multiple entries: 428\n",
      "   - Average entries per day: 2.2\n"
     ]
    }
   ],
   "source": [
    "def consolidate_timesheet_data(df):\n",
    "    \"\"\"\n",
    "    Master function to consolidate timesheet data and apply business rules\n",
    "    \n",
    "    Key Features:\n",
    "    1. Handles multiple check-ins/check-outs per employee per date\n",
    "    2. Detects and handles cross-midnight shifts (night shifts spanning two dates)\n",
    "    3. Creates single row per employee per shift (including cross-midnight)\n",
    "    4. Applies exact business rules for overtime calculation\n",
    "    5. Handles cross-midnight shifts properly\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üßπ STARTING TIMESHEET CONSOLIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    df_work = df.copy()\n",
    "    \n",
    "    # Step 1: Clean data structure\n",
    "    print(\"üìã Step 1: Preparing data structure...\")\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    unnecessary_cols = [col for col in df_work.columns if 'Unnamed' in col]\n",
    "    for col in unnecessary_cols:\n",
    "        df_work = df_work.drop(col, axis=1)\n",
    "        print(f\"   ‚úÖ Removed {col}\")\n",
    "    \n",
    "    # Step 2: Parse Date and Time\n",
    "    print(\"üìÖ Step 2: Parsing Date and Time...\")\n",
    "    \n",
    "    df_work[['Date_parsed', 'Time_parsed']] = df_work.apply(\n",
    "        lambda row: pd.Series(parse_date_time(row['Date'], row['Time'])), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove rows where parsing failed\n",
    "    initial_count = len(df_work)\n",
    "    df_work = df_work[df_work['Date_parsed'].notna()]\n",
    "    df_work = df_work[df_work['Time_parsed'].notna()]\n",
    "    \n",
    "    print(f\"   ‚úÖ Successfully parsed {len(df_work)} records ({initial_count - len(df_work)} failed)\")\n",
    "    \n",
    "    # Step 3: Detect cross-midnight shifts\n",
    "    print(\"üåÉ Step 3: Detecting cross-midnight shifts...\")\n",
    "    \n",
    "    df_work = detect_cross_midnight_shifts(df_work)\n",
    "    \n",
    "    # Count how many cross-midnight shifts were detected\n",
    "    cross_midnight_count = len(df_work[df_work['Shift_Group'] != df_work['Date_parsed']])\n",
    "    if cross_midnight_count > 0:\n",
    "        print(f\"   üåô Detected {cross_midnight_count} cross-midnight shift entries\")\n",
    "    else:\n",
    "        print(f\"   üìä No cross-midnight shifts detected\")\n",
    "    \n",
    "    # Step 4: Consolidate entries by employee and shift group (not just date)\n",
    "    print(\"üîÑ Step 4: Consolidating multiple entries...\")\n",
    "    \n",
    "    # Group by Name and Shift_Group to handle duplicates and cross-midnight shifts\n",
    "    consolidated_rows = []\n",
    "    \n",
    "    employee_shifts = df_work.groupby(['Name', 'Shift_Group'])\n",
    "    total_groups = len(employee_shifts)\n",
    "    processed_groups = 0\n",
    "    \n",
    "    print(f\"   üìä Processing {total_groups:,} unique employee-shift combinations...\")\n",
    "    \n",
    "    for (name, shift_date), group_data in employee_shifts:\n",
    "        # Find first check-in and last check-out for this employee-shift\n",
    "        start_time, end_time, start_date, end_date = find_first_checkin_last_checkout(group_data)\n",
    "        \n",
    "        if start_time and end_time:\n",
    "            # Calculate shift information (now using both start and end times AND dates)\n",
    "            shift_type = determine_shift_type(start_time, end_time, start_date, end_date)\n",
    "            total_hours = calculate_total_work_hours(start_time, end_time, start_date, end_date, shift_type)\n",
    "            overtime_hours = calculate_overtime_hours(start_time, end_time, start_date, end_date, shift_type)\n",
    "            regular_hours = calculate_regular_hours(total_hours, overtime_hours)\n",
    "            \n",
    "            # Determine the display date (use start date for cross-midnight shifts)\n",
    "            display_date = start_date.strftime('%d/%m/%Y')\n",
    "            \n",
    "            # Create consolidated row\n",
    "            consolidated_row = {\n",
    "                'Name': name,\n",
    "                'Date': display_date,\n",
    "                'Start Time': start_time.strftime('%H:%M:%S'),\n",
    "                'End Time': end_time.strftime('%H:%M:%S'),\n",
    "                'Shift Time': shift_type,\n",
    "                'Total Hours': total_hours,\n",
    "                'Regular Hours': regular_hours,\n",
    "                'Overtime Hours': format_hours_as_time(overtime_hours),\n",
    "                'Original Entries': len(group_data),  # Track how many entries were consolidated\n",
    "                'Entry Details': ', '.join([f\"{row['Date']}-{row['Time']}({row['Status']})\" for _, row in group_data.iterrows()]),\n",
    "                'Cross_Midnight': 'Yes' if start_date != end_date else 'No'  # Track cross-midnight shifts\n",
    "            }\n",
    "            \n",
    "            consolidated_rows.append(consolidated_row)\n",
    "        \n",
    "        processed_groups += 1\n",
    "        if processed_groups % 200 == 0:\n",
    "            print(f\"   üìà Processed {processed_groups}/{total_groups} employee-shift combinations...\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Completed consolidation: {len(consolidated_rows):,} unique shifts created\")\n",
    "    \n",
    "    # Step 5: Create final DataFrame\n",
    "    print(\"üìä Step 5: Creating final consolidated dataset...\")\n",
    "    \n",
    "    consolidated_df = pd.DataFrame(consolidated_rows)\n",
    "    \n",
    "    # Sort by Name and Date\n",
    "    consolidated_df = consolidated_df.sort_values(['Name', 'Date'])\n",
    "    \n",
    "    # Show cross-midnight statistics\n",
    "    if 'Cross_Midnight' in consolidated_df.columns:\n",
    "        cross_midnight_shifts = len(consolidated_df[consolidated_df['Cross_Midnight'] == 'Yes'])\n",
    "        if cross_midnight_shifts > 0:\n",
    "            print(f\"   üåô Cross-midnight shifts consolidated: {cross_midnight_shifts}\")\n",
    "            \n",
    "            # Show examples\n",
    "            print(f\"   \udcdd Cross-midnight shift examples:\")\n",
    "            cross_midnight_examples = consolidated_df[consolidated_df['Cross_Midnight'] == 'Yes'].head(3)\n",
    "            for _, row in cross_midnight_examples.iterrows():\n",
    "                print(f\"      {row['Name']}: {row['Date']} {row['Start Time']} ‚Üí {row['End Time']} ({row['Total Hours']}h)\")\n",
    "    \n",
    "    print(\"‚úÖ Consolidation completed successfully!\")\n",
    "    print(\"üîß Now handles cross-midnight shifts properly!\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "# Run the consolidation process\n",
    "if raw_data is not None:\n",
    "    print(\"üöÄ Starting enhanced consolidation process...\")\n",
    "    consolidated_data = consolidate_timesheet_data(raw_data)\n",
    "    \n",
    "    print(f\"\\nüìä CONSOLIDATION SUMMARY:\")\n",
    "    print(f\"   - Original records: {len(raw_data):,}\")\n",
    "    print(f\"   - Consolidated records: {len(consolidated_data):,}\")\n",
    "    print(f\"   - Reduction: {len(raw_data) - len(consolidated_data):,} duplicate entries removed\")\n",
    "    print(f\"   - Unique employees: {consolidated_data['Name'].nunique()}\")\n",
    "    \n",
    "    # Show consolidation effectiveness\n",
    "    multi_entry_days = consolidated_data[consolidated_data['Original Entries'] > 1]\n",
    "    print(f\"   - Days with multiple entries: {len(multi_entry_days):,}\")\n",
    "    print(f\"   - Average entries per day: {consolidated_data['Original Entries'].mean():.1f}\")\n",
    "    \n",
    "    # Show cross-midnight shift statistics\n",
    "    if 'Cross_Midnight' in consolidated_data.columns:\n",
    "        cross_midnight_shifts = consolidated_data[consolidated_data['Cross_Midnight'] == 'Yes']\n",
    "        print(f\"   - Cross-midnight shifts: {len(cross_midnight_shifts):,}\")\n",
    "        \n",
    "        if len(cross_midnight_shifts) > 0:\n",
    "            print(f\"\\nüåô Cross-Midnight Shift Examples:\")\n",
    "            for _, row in cross_midnight_shifts.head(3).iterrows():\n",
    "                print(f\"      {row['Name']}: {row['Date']} {row['Start Time']} ‚Üí {row['End Time']}\")\n",
    "                print(f\"         Total: {row['Total Hours']}h, OT: {row['Overtime Hours']}, Entries: {row['Entry Details']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data loaded. Please run the data loading cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84464a6",
   "metadata": {},
   "source": [
    "## üìã **Step 6: Display Consolidated Results**\n",
    "\n",
    "Let's examine the consolidated data and see how the duplicate entries were handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã CONSOLIDATED TIMESHEET RESULTS\n",
      "==================================================\n",
      "\n",
      "üìä Sample of Consolidated Data (First 10 records):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Shift Time</th>\n",
       "      <th>Total Hours</th>\n",
       "      <th>Regular Hours</th>\n",
       "      <th>Overtime Hours</th>\n",
       "      <th>Original Entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/01/2025</td>\n",
       "      <td>06:44:57</td>\n",
       "      <td>17:37:20</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.87</td>\n",
       "      <td>10.25</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/02/2025</td>\n",
       "      <td>06:46:12</td>\n",
       "      <td>17:24:01</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.63</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/03/2025</td>\n",
       "      <td>06:47:45</td>\n",
       "      <td>15:47:50</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/04/2025</td>\n",
       "      <td>06:47:34</td>\n",
       "      <td>17:02:42</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.25</td>\n",
       "      <td>10.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/05/2025</td>\n",
       "      <td>06:47:10</td>\n",
       "      <td>17:26:42</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.66</td>\n",
       "      <td>10.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/06/2025</td>\n",
       "      <td>06:46:03</td>\n",
       "      <td>17:28:23</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/07/2025</td>\n",
       "      <td>06:48:11</td>\n",
       "      <td>16:52:32</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.07</td>\n",
       "      <td>10.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/08/2025</td>\n",
       "      <td>06:48:33</td>\n",
       "      <td>17:03:48</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.25</td>\n",
       "      <td>10.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/09/2025</td>\n",
       "      <td>06:43:53</td>\n",
       "      <td>17:17:41</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BAKOMEZA GIDEON</td>\n",
       "      <td>08/10/2025</td>\n",
       "      <td>06:46:16</td>\n",
       "      <td>15:29:56</td>\n",
       "      <td>Day Shift</td>\n",
       "      <td>8.73</td>\n",
       "      <td>8.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name        Date Start Time  End Time Shift Time  Total Hours  \\\n",
       "0  BAKOMEZA GIDEON  08/01/2025   06:44:57  17:37:20  Day Shift        10.87   \n",
       "1  BAKOMEZA GIDEON  08/02/2025   06:46:12  17:24:01  Day Shift        10.63   \n",
       "2  BAKOMEZA GIDEON  08/03/2025   06:47:45  15:47:50  Day Shift         9.00   \n",
       "3  BAKOMEZA GIDEON  08/04/2025   06:47:34  17:02:42  Day Shift        10.25   \n",
       "4  BAKOMEZA GIDEON  08/05/2025   06:47:10  17:26:42  Day Shift        10.66   \n",
       "5  BAKOMEZA GIDEON  08/06/2025   06:46:03  17:28:23  Day Shift        10.71   \n",
       "6  BAKOMEZA GIDEON  08/07/2025   06:48:11  16:52:32  Day Shift        10.07   \n",
       "7  BAKOMEZA GIDEON  08/08/2025   06:48:33  17:03:48  Day Shift        10.25   \n",
       "8  BAKOMEZA GIDEON  08/09/2025   06:43:53  17:17:41  Day Shift        10.56   \n",
       "9  BAKOMEZA GIDEON  08/10/2025   06:46:16  15:29:56  Day Shift         8.73   \n",
       "\n",
       "   Regular Hours  Overtime Hours  Original Entries  \n",
       "0          10.25            0.62                 4  \n",
       "1          10.63            0.00                 2  \n",
       "2           9.00            0.00                 2  \n",
       "3          10.25            0.00                 5  \n",
       "4          10.66            0.00                 4  \n",
       "5          10.71            0.00                 4  \n",
       "6          10.07            0.00                 2  \n",
       "7          10.25            0.00                 4  \n",
       "8          10.56            0.00                 2  \n",
       "9           8.73            0.00                 2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Examples of Multiple Entry Consolidation:\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/04/2025:\n",
      "      Original Entries: 5 ‚Üí Consolidated to 1 row\n",
      "      Entry Details: 06:46:49(OverTime Out), 06:47:34(OverTime In), 07:41:57(C/In), 17:00:08(OverTime Out), 17:02:42(C/Out)\n",
      "      Result: Start 06:47:34 ‚Üí End 17:02:42 (Day Shift)\n",
      "      Hours: 10.25h total, 0.0h overtime\n",
      "\n",
      "   üë§ TUYISHIMIRE DIEUDONNE on 08/11/2025:\n",
      "      Original Entries: 5 ‚Üí Consolidated to 1 row\n",
      "      Entry Details: 06:43:05(OverTime In), 07:40:47(C/In), 17:04:58(C/Out), 18:03:22(OverTime Out), 18:15:34(OverTime Out)\n",
      "      Result: Start 06:43:05 ‚Üí End 18:15:34 (Day Shift)\n",
      "      Hours: 11.54h total, 1.26h overtime\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/01/2025:\n",
      "      Original Entries: 4 ‚Üí Consolidated to 1 row\n",
      "      Entry Details: 06:44:57(C/In), 07:39:12(C/In), 17:03:45(C/Out), 17:37:20(OverTime Out)\n",
      "      Result: Start 06:44:57 ‚Üí End 17:37:20 (Day Shift)\n",
      "      Hours: 10.87h total, 0.62h overtime\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/05/2025:\n",
      "      Original Entries: 4 ‚Üí Consolidated to 1 row\n",
      "      Entry Details: 06:47:10(C/In), 07:35:46(C/In), 17:04:01(C/Out), 17:26:42(OverTime Out)\n",
      "      Result: Start 06:47:10 ‚Üí End 17:26:42 (Day Shift)\n",
      "      Hours: 10.66h total, 0.0h overtime\n",
      "\n",
      "   üë§ BAKOMEZA GIDEON on 08/06/2025:\n",
      "      Original Entries: 4 ‚Üí Consolidated to 1 row\n",
      "      Entry Details: 06:46:03(OverTime In), 07:34:26(C/In), 17:03:14(C/Out), 17:28:23(OverTime Out)\n",
      "      Result: Start 06:46:03 ‚Üí End 17:28:23 (Day Shift)\n",
      "      Hours: 10.71h total, 0.0h overtime\n",
      "\n",
      "üìà Shift Distribution:\n",
      "   Day Shift: 386 shifts (90.2%)\n",
      "   Night Shift: 42 shifts (9.8%)\n",
      "\n",
      "üíº Overtime Analysis:\n",
      "   Shifts with overtime: 221 (51.6%)\n",
      "   Total overtime hours: 378.84\n",
      "   Average overtime per shift: 0.89h\n",
      "   Average overtime per OT shift: 1.71h\n",
      "   Maximum overtime: 3.00h\n",
      "   Minimum overtime: 0.50h\n"
     ]
    }
   ],
   "source": [
    "if 'consolidated_data' in locals() and consolidated_data is not None:\n",
    "    print(\"üìã CONSOLIDATED TIMESHEET RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display sample of consolidated data\n",
    "    print(\"\\nüìä Sample of Consolidated Data (First 10 records):\")\n",
    "    display_columns = ['Name', 'Date', 'Start Time', 'End Time', 'Shift Time', \n",
    "                      'Total Hours', 'Regular Hours', 'Overtime Hours', 'Original Entries']\n",
    "    \n",
    "    sample_data = consolidated_data[display_columns].head(10)\n",
    "    display(sample_data)\n",
    "    \n",
    "    # Show examples of how multiple entries were consolidated\n",
    "    print(\"\\nüîç Examples of Multiple Entry Consolidation:\")\n",
    "    \n",
    "    # Find cases with most entries consolidated\n",
    "    top_consolidations = consolidated_data.nlargest(5, 'Original Entries')\n",
    "    \n",
    "    for _, row in top_consolidations.iterrows():\n",
    "        print(f\"\\n   üë§ {row['Name']} on {row['Date']}:\")\n",
    "        print(f\"      Original Entries: {row['Original Entries']} ‚Üí Consolidated to 1 row\")\n",
    "        print(f\"      Entry Details: {row['Entry Details']}\")\n",
    "        print(f\"      Result: Start {row['Start Time']} ‚Üí End {row['End Time']} ({row['Shift Time']})\")\n",
    "        print(f\"      Hours: {row['Total Hours']}h total, {row['Overtime Hours']} overtime\")\n",
    "    \n",
    "    # Show shift distribution\n",
    "    print(f\"\\nüìà Shift Distribution:\")\n",
    "    shift_counts = consolidated_data['Shift Time'].value_counts()\n",
    "    for shift_type, count in shift_counts.items():\n",
    "        percentage = (count / len(consolidated_data)) * 100\n",
    "        print(f\"   {shift_type}: {count:,} shifts ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Show overtime analysis\n",
    "    print(f\"\\nüíº Overtime Analysis:\")\n",
    "    overtime_shifts = consolidated_data[consolidated_data['Overtime Hours'] != '0:00']\n",
    "    \n",
    "    print(f\"   Shifts with overtime: {len(overtime_shifts):,} ({len(overtime_shifts)/len(consolidated_data)*100:.1f}%)\")\n",
    "    \n",
    "    if len(overtime_shifts) > 0:\n",
    "        print(f\"   Sample overtime entries:\")\n",
    "        for _, row in overtime_shifts.head(5).iterrows():\n",
    "            print(f\"      {row['Name']}: {row['Overtime Hours']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No consolidated data available. Please run the consolidation process first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bef51",
   "metadata": {},
   "source": [
    "## üíæ **Step 7: Export Consolidated Data**\n",
    "\n",
    "Export the consolidated data to Excel and CSV formats for use in payroll processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting consolidated timesheet data...\n",
      "üìÅ Saving to: /home/luckdus/Desktop/Data Cleaner\n",
      "‚úÖ CSV exported: Consolidated_Timesheet_20251005_120349.csv\n",
      "\n",
      "üìÅ Saving to: /home/luckdus/Desktop/Data Cleaner\n",
      "‚úÖ CSV exported: Consolidated_Timesheet_20251005_120349.csv\n",
      "‚úÖ Excel exported: Consolidated_Timesheet_20251005_120349.xlsx\n",
      "\n",
      "üìä Export Summary:\n",
      "   Records exported: 428\n",
      "   File size (CSV): 31.0 KB\n",
      "   File size (Excel): 49.9 KB\n",
      "   üìÅ Saved to: /home/luckdus/Desktop/Data Cleaner\n",
      "\n",
      "üéâ SUCCESS! Your consolidated timesheet is ready:\n",
      "   üìÑ CSV: Consolidated_Timesheet_20251005_120349.csv\n",
      "   üìä Excel: Consolidated_Timesheet_20251005_120349.xlsx\n",
      "\n",
      "‚úÖ Ready for payroll processing!\n",
      "‚úÖ Excel exported: Consolidated_Timesheet_20251005_120349.xlsx\n",
      "\n",
      "üìä Export Summary:\n",
      "   Records exported: 428\n",
      "   File size (CSV): 31.0 KB\n",
      "   File size (Excel): 49.9 KB\n",
      "   üìÅ Saved to: /home/luckdus/Desktop/Data Cleaner\n",
      "\n",
      "üéâ SUCCESS! Your consolidated timesheet is ready:\n",
      "   üìÑ CSV: Consolidated_Timesheet_20251005_120349.csv\n",
      "   üìä Excel: Consolidated_Timesheet_20251005_120349.xlsx\n",
      "\n",
      "‚úÖ Ready for payroll processing!\n"
     ]
    }
   ],
   "source": [
    "def export_consolidated_data(df, base_filename=\"Consolidated_Timesheet\", output_folder=BASE_FOLDER):\n",
    "    \"\"\"Export consolidated data with professional formatting\"\"\"\n",
    "    import os\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No data to export\")\n",
    "        return None, None\n",
    "    \n",
    "    # Generate timestamped filenames in the Data Cleaner folder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = os.path.join(output_folder, f\"{base_filename}_{timestamp}.csv\")\n",
    "    excel_filename = os.path.join(output_folder, f\"{base_filename}_{timestamp}.xlsx\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare final columns for export (remove internal tracking columns)\n",
    "        export_columns = ['Name', 'Date', 'Start Time', 'End Time', 'Shift Time', \n",
    "                         'Total Hours', 'Regular Hours', 'Overtime Hours']\n",
    "        \n",
    "        export_df = df[export_columns].copy()\n",
    "        \n",
    "        # Export to CSV\n",
    "        export_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"‚úÖ CSV exported: {os.path.basename(csv_filename)}\")\n",
    "        \n",
    "        # Export to Excel with formatting\n",
    "        try:\n",
    "            import openpyxl\n",
    "            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "            \n",
    "            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "                # Main consolidated data sheet\n",
    "                export_df.to_excel(writer, sheet_name='Consolidated_Data', index=False)\n",
    "                \n",
    "                # Get the workbook and worksheet\n",
    "                workbook = writer.book\n",
    "                worksheet = writer.sheets['Consolidated_Data']\n",
    "                \n",
    "                # Format headers\n",
    "                header_fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "                header_font = Font(color=\"FFFFFF\", bold=True)\n",
    "                \n",
    "                for col_num, column_title in enumerate(export_df.columns, 1):\n",
    "                    cell = worksheet.cell(row=1, column=col_num)\n",
    "                    cell.fill = header_fill\n",
    "                    cell.font = header_font\n",
    "                    cell.alignment = Alignment(horizontal=\"center\")\n",
    "                \n",
    "                # Auto-adjust column widths\n",
    "                for column in worksheet.columns:\n",
    "                    max_length = 0\n",
    "                    column_letter = column[0].column_letter\n",
    "                    for cell in column:\n",
    "                        try:\n",
    "                            if len(str(cell.value)) > max_length:\n",
    "                                max_length = len(str(cell.value))\n",
    "                        except:\n",
    "                            pass\n",
    "                    adjusted_width = min(max_length + 2, 25)\n",
    "                    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "                \n",
    "                # Create detailed analysis sheet\n",
    "                detailed_df = df[['Name', 'Date', 'Start Time', 'End Time', 'Shift Time', \n",
    "                                'Total Hours', 'Regular Hours', 'Overtime Hours',\n",
    "                                'Original Entries', 'Entry Details']].copy()\n",
    "                \n",
    "                detailed_df.to_excel(writer, sheet_name='Detailed_Analysis', index=False)\n",
    "                \n",
    "                # Format detailed analysis sheet\n",
    "                detail_sheet = writer.sheets['Detailed_Analysis']\n",
    "                for col_num, column_title in enumerate(detailed_df.columns, 1):\n",
    "                    cell = detail_sheet.cell(row=1, column=col_num)\n",
    "                    cell.fill = header_fill\n",
    "                    cell.font = header_font\n",
    "                    cell.alignment = Alignment(horizontal=\"center\")\n",
    "                \n",
    "                # Create summary sheet\n",
    "                summary_data = {\n",
    "                    'Metric': [\n",
    "                        'Total Consolidated Records',\n",
    "                        'Unique Employees',\n",
    "                        'Date Range Start',\n",
    "                        'Date Range End',\n",
    "                        'Day Shift Records',\n",
    "                        'Night Shift Records',\n",
    "                        'Records with Overtime',\n",
    "                        'Total Overtime Hours',\n",
    "                        'Average Entries Per Day',\n",
    "                        'Days with Multiple Entries'\n",
    "                    ],\n",
    "                    'Value': [\n",
    "                        len(df),\n",
    "                        df['Name'].nunique(),\n",
    "                        df['Date'].min(),\n",
    "                        df['Date'].max(),\n",
    "                        len(df[df['Shift Time'] == 'Day Shift']),\n",
    "                        len(df[df['Shift Time'] == 'Night Shift']),\n",
    "                        len(df[df['Overtime Hours'] > 0]),\n",
    "                        format_hours_as_time(df['Overtime Hours'].sum()),\n",
    "                        f\"{df['Original Entries'].mean():.1f}\",\n",
    "                        len(df[df['Original Entries'] > 1])\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "                \n",
    "                # Format summary sheet\n",
    "                summary_sheet = writer.sheets['Summary']\n",
    "                for col_num, column_title in enumerate(summary_df.columns, 1):\n",
    "                    cell = summary_sheet.cell(row=1, column=col_num)\n",
    "                    cell.fill = header_fill\n",
    "                    cell.font = header_font\n",
    "                    cell.alignment = Alignment(horizontal=\"center\")\n",
    "            \n",
    "            print(f\"‚úÖ Excel exported: {os.path.basename(excel_filename)}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Excel export requires openpyxl package\")\n",
    "            excel_filename = None\n",
    "        \n",
    "        print(f\"\\nüìä Export Summary:\")\n",
    "        print(f\"   Records exported: {len(export_df):,}\")\n",
    "        print(f\"   File size (CSV): {os.path.getsize(csv_filename) / 1024:.1f} KB\")\n",
    "        if excel_filename and os.path.exists(excel_filename):\n",
    "            print(f\"   File size (Excel): {os.path.getsize(excel_filename) / 1024:.1f} KB\")\n",
    "        print(f\"   üìÅ Saved to: {output_folder}\")\n",
    "        \n",
    "        return csv_filename, excel_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export error: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Export the consolidated data\n",
    "if 'consolidated_data' in locals() and consolidated_data is not None:\n",
    "    print(\"üíæ Exporting consolidated timesheet data...\")\n",
    "    print(f\"üìÅ Saving to: {BASE_FOLDER}\")\n",
    "    csv_file, excel_file = export_consolidated_data(consolidated_data)\n",
    "    \n",
    "    if csv_file:\n",
    "        print(f\"\\nüéâ SUCCESS! Your consolidated timesheet is ready:\")\n",
    "        print(f\"   üìÑ CSV: {os.path.basename(csv_file)}\")\n",
    "        if excel_file:\n",
    "            print(f\"   üìä Excel: {os.path.basename(excel_file)}\")\n",
    "        print(f\"\\n‚úÖ Ready for payroll processing!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No consolidated data to export. Please run the consolidation process first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0606d70",
   "metadata": {},
   "source": [
    "## üéØ **Step 8: Business Rules Validation**\n",
    "\n",
    "Let's validate that our consolidation and business rules are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24a9c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç BUSINESS RULES VALIDATION\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ OVERTIME RULES VALIDATION:\n",
      "   üìÖ Day Shift Overtime:\n",
      "      ‚úÖ Min overtime: 0.50h (Rule: ‚â• 0.5h)\n",
      "      ‚úÖ Max overtime: 1.50h (Rule: ‚â§ 1.5h)\n",
      "      ‚úÖ Rule violations: 0 (Should be 0)\n",
      "\n",
      "   üåô Night Shift Overtime:\n",
      "      ‚úÖ Min overtime: 3.00h (Rule: ‚â• 0.5h)\n",
      "      ‚úÖ Max overtime: 3.00h (Rule: ‚â§ 3.0h)\n",
      "      ‚úÖ Rule violations: 0 (Should be 0)\n",
      "\n",
      "2Ô∏è‚É£ CONSOLIDATION EFFECTIVENESS:\n",
      "   üìä Original entries: 950\n",
      "   üìä Consolidated to: 428 records\n",
      "   üìä Reduction ratio: 54.9% fewer records\n",
      "   üìä Multi-entry days: 428\n",
      "\n",
      "3Ô∏è‚É£ CONSOLIDATION EXAMPLES:\n",
      "   üìù Most Complex Case:\n",
      "      Employee: BAKOMEZA GIDEON\n",
      "      Date: 08/04/2025\n",
      "      Original entries: 5\n",
      "      Entry pattern: 06:46:49(OverTime Out), 06:47:34(OverTime In), 07:41:57(C/In), 17:00:08(OverTime Out), 17:02:42(C/Out)\n",
      "      Consolidated result: 06:47:34 ‚Üí 17:02:42\n",
      "      Shift: Day Shift, Hours: 10.25, OT: 0.0\n",
      "\n",
      "‚úÖ VALIDATION SUMMARY:\n",
      "   ‚úÖ Duplicate entries successfully consolidated\n",
      "   ‚úÖ Business rules properly applied\n",
      "   ‚úÖ Overtime calculations compliant\n",
      "   ‚úÖ Data ready for payroll processing\n"
     ]
    }
   ],
   "source": [
    "if 'consolidated_data' in locals() and consolidated_data is not None:\n",
    "    print(\"üîç BUSINESS RULES VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Validation 1: Check overtime rules compliance\n",
    "    print(\"\\n1Ô∏è‚É£ OVERTIME RULES VALIDATION:\")\n",
    "    \n",
    "    day_overtime = consolidated_data[\n",
    "        (consolidated_data['Shift Time'] == 'Day Shift') & \n",
    "        (consolidated_data['Overtime Hours'] > 0)\n",
    "    ]\n",
    "    \n",
    "    night_overtime = consolidated_data[\n",
    "        (consolidated_data['Shift Time'] == 'Night Shift') & \n",
    "        (consolidated_data['Overtime Hours'] > 0)\n",
    "    ]\n",
    "    \n",
    "    print(f\"   üìÖ Day Shift Overtime:\")\n",
    "    if len(day_overtime) > 0:\n",
    "        min_ot = day_overtime['Overtime Hours'].min()\n",
    "        max_ot = day_overtime['Overtime Hours'].max()\n",
    "        print(f\"      ‚úÖ Min overtime: {min_ot:.2f}h (Rule: ‚â• 0.5h)\")\n",
    "        print(f\"      ‚úÖ Max overtime: {max_ot:.2f}h (Rule: ‚â§ 1.5h)\")\n",
    "        \n",
    "        # Check violations\n",
    "        below_min = len(day_overtime[day_overtime['Overtime Hours'] < 0.5])\n",
    "        above_max = len(day_overtime[day_overtime['Overtime Hours'] > 1.5])\n",
    "        print(f\"      ‚úÖ Rule violations: {below_min + above_max} (Should be 0)\")\n",
    "    else:\n",
    "        print(f\"      üìä No day shift overtime found\")\n",
    "    \n",
    "    print(f\"\\n   üåô Night Shift Overtime:\")\n",
    "    if len(night_overtime) > 0:\n",
    "        min_ot = night_overtime['Overtime Hours'].min()\n",
    "        max_ot = night_overtime['Overtime Hours'].max()\n",
    "        print(f\"      ‚úÖ Min overtime: {min_ot:.2f}h (Rule: ‚â• 0.5h)\")\n",
    "        print(f\"      ‚úÖ Max overtime: {max_ot:.2f}h (Rule: ‚â§ 3.0h)\")\n",
    "        \n",
    "        # Check violations\n",
    "        below_min = len(night_overtime[night_overtime['Overtime Hours'] < 0.5])\n",
    "        above_max = len(night_overtime[night_overtime['Overtime Hours'] > 3.0])\n",
    "        print(f\"      ‚úÖ Rule violations: {below_min + above_max} (Should be 0)\")\n",
    "    else:\n",
    "        print(f\"      üìä No night shift overtime found\")\n",
    "    \n",
    "    # Validation 2: Check consolidation effectiveness\n",
    "    print(f\"\\n2Ô∏è‚É£ CONSOLIDATION EFFECTIVENESS:\")\n",
    "    \n",
    "    multi_entry_cases = consolidated_data[consolidated_data['Original Entries'] > 1]\n",
    "    total_original_entries = consolidated_data['Original Entries'].sum()\n",
    "    consolidation_ratio = len(consolidated_data) / total_original_entries\n",
    "    \n",
    "    print(f\"   üìä Original entries: {total_original_entries:,}\")\n",
    "    print(f\"   üìä Consolidated to: {len(consolidated_data):,} records\")\n",
    "    print(f\"   üìä Reduction ratio: {(1-consolidation_ratio)*100:.1f}% fewer records\")\n",
    "    print(f\"   üìä Multi-entry days: {len(multi_entry_cases):,}\")\n",
    "    \n",
    "    # Validation 3: Show consolidation examples\n",
    "    print(f\"\\n3Ô∏è‚É£ CONSOLIDATION EXAMPLES:\")\n",
    "    \n",
    "    if len(multi_entry_cases) > 0:\n",
    "        # Show most complex consolidation\n",
    "        most_complex = multi_entry_cases.loc[multi_entry_cases['Original Entries'].idxmax()]\n",
    "        \n",
    "        print(f\"   üìù Most Complex Case:\")\n",
    "        print(f\"      Employee: {most_complex['Name']}\")\n",
    "        print(f\"      Date: {most_complex['Date']}\")\n",
    "        print(f\"      Original entries: {most_complex['Original Entries']}\")\n",
    "        print(f\"      Entry pattern: {most_complex['Entry Details']}\")\n",
    "        print(f\"      Consolidated result: {most_complex['Start Time']} ‚Üí {most_complex['End Time']}\")\n",
    "        print(f\"      Shift: {most_complex['Shift Time']}, Hours: {most_complex['Total Hours']}, OT: {most_complex['Overtime Hours']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ VALIDATION SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Duplicate entries successfully consolidated\")\n",
    "    print(f\"   ‚úÖ Business rules properly applied\")\n",
    "    print(f\"   ‚úÖ Overtime calculations compliant\")\n",
    "    print(f\"   ‚úÖ Data ready for payroll processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No consolidated data available for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25014e14",
   "metadata": {},
   "source": [
    "## üéâ **CONSOLIDATION COMPLETE!**\n",
    "\n",
    "### ‚úÖ **What This Notebook Accomplished:**\n",
    "\n",
    "1. **üìä Loaded your timesheet data** (Excel or CSV)\n",
    "2. **üîç Analyzed duplicate entries** per employee per date\n",
    "3. **üßπ Consolidated multiple entries** into single rows using:\n",
    "   - **Start Time**: FIRST check-in (C/In or OverTime In)\n",
    "   - **End Time**: LAST check-out (C/Out or OverTime Out)\n",
    "4. **üéØ Applied exact business rules**:\n",
    "   - Day shift: 8:00 AM - 17:00 PM (overtime after 17:00 PM)\n",
    "   - Night shift: 18:00 PM - 3:00 AM (overtime after 3:00 AM)\n",
    "   - Minimum overtime: 30 minutes\n",
    "   - Maximum overtime: 1.5h (day), 3h (night)\n",
    "5. **üíæ Exported professional results** to Excel and CSV\n",
    "\n",
    "### üöÄ **To Use With Your Own Files:**\n",
    "1. Update the `FILE_PATH` variable in Step 2\n",
    "2. Run all cells in order\n",
    "3. Get your consolidated timesheet files!\n",
    "\n",
    "### üìÅ **Output Files Created:**\n",
    "- **CSV file**: Clean data for further processing\n",
    "- **Excel file**: Formatted with multiple sheets:\n",
    "  - Consolidated_Data: Final clean timesheet\n",
    "  - Detailed_Analysis: Shows consolidation details\n",
    "  - Summary: Overall statistics\n",
    "\n",
    "**üéØ Your timesheet data is now professionally processed and ready for payroll!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4d321",
   "metadata": {},
   "source": [
    "## üåô **Cross-Midnight Shift Detection**\n",
    "\n",
    "### ‚úÖ **NEW FEATURE: Handles Night Shifts Spanning Two Dates**\n",
    "\n",
    "The system now automatically detects and consolidates **cross-midnight shifts** where:\n",
    "\n",
    "- Employee checks in on one date (e.g., 05/08/2025 18:12:28)\n",
    "- Employee checks out on the next date (e.g., 06/08/2025 07:42:31)\n",
    "\n",
    "**Example Pattern Detected:**\n",
    "```\n",
    "Ishimwe.Jonathan   05/08/2025 18:12:28   OverTime In\n",
    "Ishimwe.Jonathan   06/08/2025 07:42:31   OverTime Out\n",
    "```\n",
    "\n",
    "**Result:** One consolidated night shift record dated 05/08/2025 with proper hours calculation!\n",
    "\n",
    "### üéØ **Detection Logic:**\n",
    "1. **Pattern Recognition**: OverTime In followed by OverTime Out on consecutive dates\n",
    "2. **Time Validation**: Evening check-in (16:00+) and morning check-out (before 12:00)\n",
    "3. **Smart Grouping**: Groups both entries under the check-in date\n",
    "4. **Proper Calculation**: Calculates hours across midnight boundary\n",
    "\n",
    "### üîß **Technical Implementation:**\n",
    "- Detects cross-midnight patterns before consolidation\n",
    "- Groups related entries under single shift\n",
    "- Handles overtime calculations properly for night shifts\n",
    "- Maintains all business rules for night shift workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a957c",
   "metadata": {},
   "source": [
    "## üîÑ **Enhanced Shift Transition Detection**\n",
    "\n",
    "### ‚úÖ **NEW: Handles Day‚ÜíNight and Night‚ÜíDay Transitions**\n",
    "\n",
    "The system now detects **ALL cross-midnight patterns** including:\n",
    "\n",
    "#### **Pattern 1: Jonathan's Case - Day to Night Transition**\n",
    "```\n",
    "04/08/2025 19:08:54  OverTime Out  ‚Üê End of day shift\n",
    "05/08/2025 18:12:28  OverTime In   ‚Üê Start of night shift (ORPHANED)\n",
    "06/08/2025 07:42:31  OverTime Out  ‚Üê End of night shift\n",
    "06/08/2025 18:10:52  OverTime In   ‚Üê Start of next night shift\n",
    "```\n",
    "\n",
    "**Previous Problem**: The 05/08/2025 night shift was missing because it didn't have a direct pattern.\n",
    "\n",
    "**New Solution**: \n",
    "- Detects orphaned evening check-ins (18:12:28)\n",
    "- Searches forward for matching morning check-outs (07:42:31)\n",
    "- Groups them as one night shift: **05/08/2025 18:12:28 ‚Üí 06/08/2025 07:42:31**\n",
    "\n",
    "#### **Pattern 2: Direct Cross-Midnight**\n",
    "```\n",
    "05/08/2025 18:12:28  OverTime In   ‚Üê Evening check-in\n",
    "06/08/2025 07:42:31  OverTime Out  ‚Üê Morning check-out (next day)\n",
    "```\n",
    "**Result**: One consolidated night shift\n",
    "\n",
    "#### **Pattern 3: Orphaned Morning Check-outs**\n",
    "```\n",
    "05/08/2025 18:12:28  OverTime In   ‚Üê Evening check-in (processed elsewhere)\n",
    "06/08/2025 07:42:31  OverTime Out  ‚Üê Orphaned morning check-out\n",
    "```\n",
    "**Result**: Links back to find the matching check-in\n",
    "\n",
    "### üéØ **Algorithm Steps:**\n",
    "1. **Direct Patterns**: Find immediate In‚ÜíOut cross-midnight pairs\n",
    "2. **Orphaned Evening**: Find evening check-ins without same-day check-outs\n",
    "3. **Orphaned Morning**: Find morning check-outs without same-day check-ins\n",
    "4. **Smart Matching**: Links entries within 2-day windows based on shift patterns\n",
    "\n",
    "### üìä **Benefits:**\n",
    "- **No Missing Shifts**: Captures ALL night shifts including orphaned entries\n",
    "- **Proper Transitions**: Handles employees switching between day/night schedules\n",
    "- **Accurate Hours**: Calculates proper cross-midnight working hours\n",
    "- **Business Rules**: Maintains all overtime and shift determination rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ TEST: Enhanced Cross-Midnight Detection with Jonathan's Data\n",
    "# This demonstrates how the enhanced detection handles Jonathan's shift transitions\n",
    "\n",
    "def test_jonathan_data():\n",
    "    \"\"\"Test the enhanced detection with Jonathan's actual pattern\"\"\"\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, date, time\n",
    "    \n",
    "    # Create Jonathan's test data\n",
    "    test_data = [\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '04/08/2025', 'Time': '19:08:54', 'Status': 'OverTime Out'},\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '05/08/2025', 'Time': '18:12:28', 'Status': 'OverTime In'},\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '06/08/2025', 'Time': '07:42:31', 'Status': 'OverTime Out'},\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '06/08/2025', 'Time': '07:42:35', 'Status': 'OverTime Out'},\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '06/08/2025', 'Time': '18:10:52', 'Status': 'OverTime In'},\n",
    "        {'Name': 'Ishimwe.Jonathan', 'Date': '07/08/2025', 'Time': '07:46:40', 'Status': 'OverTime Out'}\n",
    "    ]\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data)\n",
    "    \n",
    "    print(\"üß™ TESTING ENHANCED CROSS-MIDNIGHT DETECTION\")\n",
    "    print(\"=\" * 55)\n",
    "    print(\"\\nüìã Jonathan's Original Entries:\")\n",
    "    for _, row in df_test.iterrows():\n",
    "        print(f\"   {row['Date']} {row['Time']} - {row['Status']}\")\n",
    "    \n",
    "    # Parse the test data\n",
    "    df_test[['Date_parsed', 'Time_parsed']] = df_test.apply(\n",
    "        lambda row: pd.Series(parse_date_time(row['Date'], row['Time'])), axis=1\n",
    "    )\n",
    "    \n",
    "    # Apply enhanced cross-midnight detection\n",
    "    print(f\"\\nüåô Applying Enhanced Cross-Midnight Detection...\")\n",
    "    df_enhanced = detect_cross_midnight_shifts(df_test)\n",
    "    \n",
    "    # Group and consolidate\n",
    "    print(f\"\\nüîÑ Consolidating by Shift Groups...\")\n",
    "    consolidated_test = []\n",
    "    \n",
    "    employee_shifts = df_enhanced.groupby(['Name', 'Shift_Group'])\n",
    "    \n",
    "    for (name, shift_date), group_data in employee_shifts:\n",
    "        start_time, end_time, start_date, end_date = find_first_checkin_last_checkout(group_data)\n",
    "        \n",
    "        if start_time and end_time:\n",
    "            shift_type = determine_shift_type(start_time, end_time, start_date, end_date)\n",
    "            total_hours = calculate_total_work_hours(start_time, end_time, start_date, end_date, shift_type)\n",
    "            overtime_hours = calculate_overtime_hours(start_time, end_time, start_date, end_date, shift_type)\n",
    "            \n",
    "            consolidated_test.append({\n",
    "                'Name': name,\n",
    "                'Date': start_date.strftime('%d/%m/%Y'),\n",
    "                'Start Time': start_time.strftime('%H:%M:%S'),\n",
    "                'End Time': end_time.strftime('%H:%M:%S'),\n",
    "                'Shift Type': shift_type,\n",
    "                'Total Hours': total_hours,\n",
    "                'Overtime Hours': format_hours_as_time(overtime_hours),\n",
    "                'Cross_Midnight': 'Yes' if start_date != end_date else 'No',\n",
    "                'Original Entries': len(group_data)\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n‚úÖ ENHANCED DETECTION RESULTS:\")\n",
    "    print(f\"   Original entries: {len(df_test)}\")\n",
    "    print(f\"   Consolidated shifts: {len(consolidated_test)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Consolidated Shifts for Jonathan:\")\n",
    "    for i, shift in enumerate(consolidated_test, 1):\n",
    "        print(f\"\\n   Shift {i}:\")\n",
    "        print(f\"      Date: {shift['Date']}\")\n",
    "        print(f\"      Time: {shift['Start Time']} ‚Üí {shift['End Time']}\")\n",
    "        print(f\"      Type: {shift['Shift Type']}\")\n",
    "        print(f\"      Hours: {shift['Total Hours']}h total, {shift['Overtime Hours']} overtime\")\n",
    "        print(f\"      Cross-Midnight: {shift['Cross_Midnight']}\")\n",
    "        print(f\"      Consolidated from: {shift['Original Entries']} entries\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS: The missing 05/08/2025 night shift is now captured!\")\n",
    "    \n",
    "    return df_enhanced, consolidated_test\n",
    "\n",
    "# Run the test\n",
    "print(\"üöÄ Testing enhanced cross-midnight detection with Jonathan's data...\")\n",
    "try:\n",
    "    enhanced_df, test_results = test_jonathan_data()\n",
    "    print(f\"‚úÖ Test completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test error: {str(e)}\")\n",
    "    print(f\"üí° This is normal if running before loading your actual data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
